# Deep Learning
A subset of machine learning using neural networks with multiple layers to learn complex patterns.

Machine learning is the base of intelligence for computers and other electronic devices. It uses predictive models that can learn from existing data and forecast future behaviors, outcomes, and trends. **Deep learning** is a sub-field of machine learning, where models inspired by the human brain are expressed mathematically. 

In Deep Neural Networks (DNN) the parameters defining the mathematical models, which can be in the order of a few thousand to 100+ million, are learned automatically from the data. DNNs can model complex non-linear relationships between inputs and outputs. Their architectures generate compositional models where the object is expressed as a layered composition of primitives. Deep architectures include many variants of a few basic approaches. 

Unlike traditional machine learning, DNN can automatically discover relevant features without manual feature engineering.

Challenges: Requires large amounts of data and computational power, models are often "black boxes," prone to overfitting, and can be sensitive to adversarial attacks.

## History

Built on neural network research from the 1940s-80s. The term "deep learning" gained prominence in the 2000s with Geoffrey Hinton's work. GPU computing and big data enabled the deep learning revolution of the 2010s.

In recent years, DNNs are widely considered and several models for different applications are proposed.

These models can be divided into five categories: [Convolution Neural Networks (CNN)](../C/convolutional), Restricted
Boltzmann Machines (RBM), Autoencoders, Sparse Coders, and [Recurrent Neural Networks](../R/recurrent-neural-network). 