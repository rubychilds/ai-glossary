
Epoch
Short Definition: One complete pass through the entire training dataset during model training.
Longer Definition: An epoch represents a full cycle where the learning algorithm has processed every example in the training dataset once. Multiple epochs are typically needed for a model to learn patterns effectively, with the number of epochs being a key hyperparameter.
Examples: Training a neural network might require 10-1000 epochs depending on the complexity of the task and dataset size.
Challenges: Determining the optimal number of epochs to avoid underfitting (too few) or overfitting (too many), computational cost of many epochs, and monitoring training progress.