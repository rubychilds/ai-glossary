Hyperparameters
Short Definition: Configuration settings that control how a machine learning algorithm learns, set before training begins.
Longer Definition: Hyperparameters are parameters that are not learned from data but are set by the practitioner to control the learning process. They affect model architecture, training behavior, and performance, requiring careful tuning for optimal results.
Examples: Learning rate, batch size, number of hidden layers, regularization strength, dropout rate, number of epochs.
Challenges: Hyperparameter tuning is time-consuming and computationally expensive, interactions between hyperparameters are complex, and optimal values often depend on the specific dataset and task.