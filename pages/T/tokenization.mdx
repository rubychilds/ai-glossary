Tokenization

Short Definition: The process of breaking down text into smaller units (tokens) that can be processed by machine learning models.
Longer Definition: Tokenization converts raw text into structured units like words, subwords, or characters that AI models can work with. Modern approaches often use subword tokenization to handle out-of-vocabulary words and reduce vocabulary size.
Examples: Word-level tokenization ("Hello world" â†’ ["Hello", "world"]), subword tokenization using BPE or SentencePiece, character-level tokenization.
Challenges: Handling different languages and scripts, dealing with punctuation and special characters, managing vocabulary size vs coverage trade-offs, and ensuring consistent tokenization across training and inference.